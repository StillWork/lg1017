{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BsFbov-mYgH"
   },
   "source": [
    "# 토픽모델링 (LDA)\n",
    "- colab에서 실행\n",
    "- 빅카인즈 뉴스기사 데이터 활용(https://www.bigkinds.or.kr/)\n",
    "- [시각화 pyLDAvis](https://blog.devgenius.io/working-with-pyldavis-topic-modeling-exploration-tool-b03682d57079)\n",
    "- [한글 시각화](https://wikidocs.net/30708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwLxNFThoTc9",
    "outputId": "fbdc92bf-6a27-4763-b224-0fd66da9518c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RAKKV201mYgJ"
   },
   "outputs": [],
   "source": [
    "import konlpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohsJ-x4nnakl",
    "outputId": "5ac23d73-4b60-4dbc-f0c4-47fb7b5a368d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  309k  100  309k    0     0   552k      0 --:--:-- --:--:-- --:--:--  553k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/StillWork/data/master/news_1500.xlsx\\\n",
    "     -o news_1500.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NWAqESDNmYgM"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('news_1500.xlsx', names=['기사'], keep_default_na=False)\n",
    "\n",
    "#Countvectorizer적용을 위해 list로 변환\n",
    "list_train = df_train['기사'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzcn7CzxnsID",
    "outputId": "3702edcb-6c52-4357-d134-5ef40416a706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-9QTOBPn0wS",
    "outputId": "cdb89473-1d95-4342-92a7-6ca44bdb2b25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['호주 브리즈번에서 발생한 한국인 워킹홀리데이 참가자 살해범이 같은 한국인 것으로 드라나 충격을 주고 있다. \\n \\n20일 호주 국영 ABC방송은 퀸즐랜드주 경찰이 김모(28)씨를 살해하고 암매장한 혐의로\\xa0한국인 홍모(28)씨를\\xa0체포했다고 보도했다. \\n \\n경찰에 따르면 홍씨는\\xa0지난 16일 오후 브리즈번 캐넌힐에서 1만 5000 호주달러를 원화로 환전해줄 사..',\n",
       " '호주 경찰이 한국인 워킹홀리데이 참가자를 살해한 혐의로 또 다른 한국인 남성을 기소했다고 호주 언론이 보도했다. \\n \\n퀸즐랜드주 경찰은 20일(현지시간) 금품을 빼앗기 위해 김민태(28)씨를 살해한 혐의로 황모(28)씨를 기소했다고 시드니모닝헤럴드가 보도했다. \\n \\n그는 이날 브리즈번 법원에서 열린 심리에 출석해 기자들로부터 고개를 돌린 채 아래쪽만 쳐..',\n",
       " '\\xa0 \\n \\n호주 괴물 가재가 공개돼 네티즌들의 폭발적인 관심을 끌고 있다. \\n \\n최근 각종 온라인 커뮤니티 게시판에는 ‘호주 괴물 가재’라는 제목의 사진 한 장이 게재됐다. \\n \\n공개된 사진에는 웬만한 성인 남성 팔 크기와 비슷한 가재의 모습이 담겨 있다. 사진 속 남성도 가재가 무거운지 엉거주춤한 자세를 취하고 있다. 호주에서 잡힌 것으로 알려진 가재의..',\n",
       " '최근 인기를 끌고 있는 tvN의 드라마 ‘응답하라 1994’가 같은 장면을 내보내고 다른 프로그램의 화면이 나오는 등 방송사고를 냈다. \\n▶ 방송사고 이후 사과 자막이 나오는 tvN ‘응답하라 1994’의 한 장면.20일 밤 10시쯤 드라마 ‘응답하라 1994’가 방송되는 도중에 방송사고가 발생했다. 이날 방송에서 성나정이 호주로 떠났다가 한국으로 돌아..',\n",
       " '호주 워킹홀리데이 한국인 참가자가 19일 또다시 숨진 채 발견됐다. 지난달 호주에서 한국인 여대생이 무참히 살해된 지 한 달도 지나지 않아 발생한 이번 사건으로 호주 교민사회 및 어학연수 희망자들의 불안감이 커지고 있다. \\n \\n호주 ABC방송 등에 따르면 퀸즐랜드주 경찰은 이날 브리즈번 남서부 앨지스터 한 주택 뒷마당에서 발견된 변사체가 사흘 전 행방불..']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fiT5TWoImYgQ"
   },
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "results = []\n",
    "for line in list_train:\n",
    "    word_list = okt.pos(line, norm=True, stem=True)\n",
    "    words = []\n",
    "    for word in word_list:\n",
    "        if not word[1] in [\"Josa\", \"eomi\", \"Punctuation\", \"Verb\"]:\n",
    "            words.append(word[0])\n",
    "    words = (\" \".join(words)).strip()\n",
    "    results.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fOJsf8t2mYgT"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=2000, max_df=.10)\n",
    "X = vect.fit_transform(results)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CDlvyGlmYgV",
    "outputId": "9bfddd4c-6973-45b9-8934-940695cb9772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xwof2b3HmYgZ"
   },
   "outputs": [],
   "source": [
    "# 토픽마다(components_의 행) 특성을 오름차순으로 정렬합니다\n",
    "# 내림차순이 되도록 [:, ::-1] 사용해 행의 정렬을 반대로 바꿉니다\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "\n",
    "# CountVectorizer 객체에서 특성 이름을 구합니다.\n",
    "feature_names = np.array(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOiDo2XumYgc",
    "outputId": "378b8263-f8ae-45ba-bb45-6a925a422dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "올림픽           보도            삼성            대통령           여성            \n",
      "박태환           필리핀           대만            정부            일리            \n",
      "리우            발생            2013          밉다            일간            \n",
      "수영            언론            시리즈           트럼프           시드니           \n",
      "출전            통신            공개            장관            소녀            \n",
      "리우데자네이루       올해            리그            회의            메일            \n",
      "남자            도시            경기            지역            외신            \n",
      "선수            영화            프로야구          국가            남성            \n",
      "국제            방송            대표            정보            보도            \n",
      "27            가장            발견            협정            같다            \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "골프            서비스           조사            축구            위해            \n",
      "투어            게임            결과            브라질           통해            \n",
      "여자            해외            수출            연맹            시드니           \n",
      "대회            유학            발표            리그            진행            \n",
      "프로            공개            해외            대표팀           인천            \n",
      "랭킹            투자            국제            전북            감독            \n",
      "lpga          결혼            개국            월드컵           오후            \n",
      "상금            우리            지난해           감독            출국            \n",
      "라운드           우리나라          가장            국제            국제공항          \n",
      "pga           통화            교수            서울            단식            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10개의 토픽을 출력합니다\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk=6, n_words=20):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        # for each chunk:\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        # maybe we have less than topics_per_chunk left\n",
    "        len_this_chunk = len(these_topics)\n",
    "        # print topic headers\n",
    "        print((\"topic {:<8}\" * len_this_chunk).format(*these_topics))\n",
    "        print((\"-------- {0:<5}\" * len_this_chunk).format(\"\"))\n",
    "        # print top n_words frequent words\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print((\"{:<14}\" * len_this_chunk).format(\n",
    "                    *feature_names[sorting[these_topics, i]]))\n",
    "            except:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "\n",
    "print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVXeKAoGmYgv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
